# -*- coding: utf-8 -*-
"""DSCI310_FDA_FAERS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zm3HyyL59BBlbiNuwHr2b1jXYh5MLvo0

# FDA FAERS Drug Safety Prediction Model

**Project Goal:** Predict adverse events from patient and drug characterists to help drug safety teams prioritze reviews.

**Dataset:** FDA FAERS Q2 2025

## Table of Contents
1. Setup and Data Loading
2. Exploratory Data Analysis
3. Data Cleaning/preparation/preprocessing
4. Feature Engineering
5. Models and Metrics
6. Results and Analysis

---
# 1. Setup and Data Loading
## 1.1 Import Libraries

Loading the Python libraries for data anlyysis, statistical modeling, and visualization
"""

#import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

#display options
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)

print('libraries loaded successfully')
print(f"Pandas version: {pd.__version__}")
print(f"Numpy version: {np.__version__}")

"""## 1.2 Load FAERS Data Tables

The FDA FAERS database is organized into 7 relational tables that link via PrimaryID. We are loading the Q2 2025 data (April - June 2025).

**Table Structure:**
- **DEMO**: Patient demographics
- **DRUG**: Medications involved
- **REAC**: Adverse reactions/symptoms
- **OUTC**: Patient outcomes - death, hospitalization, etc.
- **INDI**: Drug indications
- **THER**: Therapy start/stop dates
- **RPSR**: Report sources - who made the report(provider, patient)

**Data Format:** $-delimited text files
"""

#Load all 7 FAERS tables
"""Tarif"""

from google.colab import drive
drive.mount('/content/drive')
dataPath = '/content/drive/MyDrive/faers_ascii_2025q2/ASCII/'

#utf-8 allows us to handle special characters
#low memory allows us to read the entire file before deciding column tyoes
demo = pd.read_csv(dataPath + 'DEMO25Q2.txt', delimiter='$', encoding='utf-8', low_memory=False)
drug = pd.read_csv(dataPath + 'DRUG25Q2.txt', delimiter='$', encoding='utf-8', low_memory=False)
reac = pd.read_csv(dataPath + 'REAC25Q2.txt', delimiter='$', encoding='utf-8', low_memory=False)
outc = pd.read_csv(dataPath + 'OUTC25Q2.txt', delimiter='$', encoding='utf-8', low_memory=False)
indi = pd.read_csv(dataPath + 'INDI25Q2.txt', delimiter='$', encoding='utf-8', low_memory=False)
ther = pd.read_csv(dataPath + 'THER25Q2.txt', delimiter='$', encoding='utf-8', low_memory=False)
rpsr = pd.read_csv(dataPath + 'RPSR25Q2.txt', delimiter='$', encoding='utf-8', low_memory=False)

#Show was was loaded
tables = {
    'DEMO (Demographics)': demo,
    'DRUG (Medications)': drug,
    'REAC (Reactions)': reac,
    'OUTC (Outcomes)': outc,
    'INDI (Indications)': indi,
    'THER (Therapy Dates)': ther,
    'RPSR (Report Sources)': rpsr
}
#these tables are relational and link via PRIMARYID

print("="*60)
#loop through each table and print the size
for name, df in tables.items():
    print(f"{name:30} {len(df):,} rows * {len(df.columns)} columns")
print("="*60)

demo = pd.read_csv('DEMO25Q2.txt', encoding='utf-8', low_memory=False, sep='$', quotechar='"', on_bad_lines='skip')
drug = pd.read_csv('DRUG25Q2.txt', encoding='utf-8', low_memory=False, sep='$', quotechar='"', on_bad_lines='skip')
reac = pd.read_csv('REAC25Q2.txt', encoding='utf-8', low_memory=False, sep='$', quotechar='"', on_bad_lines='skip')
outc = pd.read_csv('OUTC25Q2.txt', encoding='utf-8', low_memory=False, sep='$', quotechar='"', on_bad_lines='skip')
indi = pd.read_csv('INDI25Q2.txt', encoding='utf-8', low_memory=False, sep='$', quotechar='"', on_bad_lines='skip')
ther = pd.read_csv('THER25Q2.txt', encoding='utf-8', low_memory=False, sep='$', quotechar='"', on_bad_lines='skip')
rpsr = pd.read_csv('RPSR25Q2.txt', encoding='utf-8', low_memory=False, sep='$', quotechar='"', on_bad_lines='skip')

#Show was was loaded
tables = {
    'DEMO (Demographics)': demo,
    'DRUG (Medications)': drug,
    'REAC (Reactions)': reac,
    'OUTC (Outcomes)': outc,
    'INDI (Indications)': indi,
    'THER (Therapy Dates)': ther,
    'RPSR (Report Sources)': rpsr
}
#these tables are relational and link via PRIMARYID

print("="*60)
#loop through each table and print the size
for name, df in tables.items():
    print(f"{name:30} {len(df):,} rows * {len(df.columns)} columns")
print("="*60)

print("First 5 lines of DEMO25Q2.txt:")
display(pd.read_csv('/content/DEMO25Q2.txt', sep='$', header=None))

print("First 5 lines of DRUG25Q2.txt:")
display(pd.read_csv('/content/DRUG25Q2.txt', sep='$', header=None))

print("First 5 lines of REAC25Q2.txt:")
display(pd.read_csv('/content/REAC25Q2.txt', sep='$', header=None).head())

print("\nFirst 5 lines of OUTC25Q2.txt:")
display(pd.read_csv('/content/OUTC25Q2.txt', sep='$', header=None).head())

print("\nFirst 5 lines of INDI25Q2.txt:")
display(pd.read_csv('/content/INDI25Q2.txt', sep='$', header=None).head())

print("\nFirst 5 lines of THER25Q2.txt:")
display(pd.read_csv('/content/THER25Q2.txt', sep='$', header=None).head())

print("\nFirst 5 lines of RPSR25Q2.txt:")
display(pd.read_csv('/content/RPSR25Q2.txt', sep='$', header=None).head())

"""**Summary:** Loaded 7 tables representing 393,130 ADE reports.
The higher row counts in tables such as the drug and reactions tables indicate that patients typically take multiple medications and multiple adverse reactions per report.

# 2. Exploratory Data Analysis

## 2.1 Demographics Table Structure

The DEMO table is the primary table contating one row per ADE report. We will examine its structure, data quality, and missing data patterns.
"""

demo.columns

drug.columns

reac.columns

outc.columns

indi.columns

ther.columns

reac.columns

#exploring the structure
print('Demographics table')
print('='*70)
print(f"Shape: {demo.shape[0]:,} rows * {demo.shape[1]} columnsn")

#get the column names
print('Column names:')
print(demo.columns.tolist())

#see the format and data types
print('\n' + '='*70)
print('Firsst 3 ADE reports')
print(demo.head(3))

#count missing data
print('\n' + '='*70)
print('Missing data summary')
print(f"{'Column':<20} {'Missing':<15} {'Percent':<10}")
print('='*70)

missing_count = demo.isnull().sum()
missing_percent = (missing_count / len(demo)*100)
for col in missing_count[missing_count > 0].sort_values(ascending=False).index:
    count = missing_count[col]
    percent = missing_percent[col]
    print(f"{col:<20} {count:>10,} {percent:>9.2f}%")
print('\n' + '='*70)
print('Columns with 0 percent missing (complete columns)')
complete_cols = missing_percent[missing_percent == 0].index.tolist()
print(complete_cols)

"""**Key Ovservations:**

**Data Quality:**
- `to_mfr`: (if the manufacturer was notified) 96.51 percent missing. This will be exluded from the analysis
- `auth_num`(authorization number): 90.58 percent missing. Will be excluded from analysis
- `wt` (weight): 82.51 percent missing. This is probably too sparse for modeling.
- `age_grp` (FDA age group categories): 63.94 percent missing. We can derive from `age` instead.
- `event_date` (event date): 55.67 percent missing. This will limit a temporal analysis (if we even do one)
- `age`: 39.68 percent missing. This is a critical limitation but we still have over 200,000 reports with age.
- `sex`: 21.45 percent missing. This is a moderate limitation.

## 2.2 Analyze Outcomes Table (Target Variable)

The OUTC table contains patient outcomes, which will be our prediction target.

Objectives:
1. Understand what the outcome codes mean
2. Count the distribution of each outcome type
3. Define 'serious' vs. 'non-serioud' outcomes for a binary classification model

The FDA defines serious outcomes as: Death, Life-Threatening, Hospitalization, Disablility, Congenital Anomaly, Required Intervention, or 'Other'. 'Other' events are ones that do not fit into the other categories, but may jeopardize the patient and require intervention, such as a seizure or allergic reaction. However, 'Other' are typically non-serious unless the context suggests otherwise, so we will not include it in our analysis of serious events
"""

#analyze the outcomes table
print('Outcomes table')
print('='*70)
print(f"Shape: {outc.shape[0]:,} rows * {outc.shape[1]} columns\n")

#column names
print('column names:')
print(outc.columns.tolist())

#look at first few outcomes to see structure
print('\n' + '='*70)
print('First 10 outcomes')
print(outc.head(10))

#count each outcome code
print('\n' + '='*70)
print('Outcome code distribution')
outcome_counts = outc['outc_cod'].value_counts()
print(outcome_counts)

#calculate the percent
outcome_percents = (outc['outc_cod'].value_counts(normalize=True) * 100).round(3)
for code, percent in outcome_percents.items():
    print(f"{code}: {percent}%")

#define the serious ones
print('\n' + '='*70)
print('Defining serious outcomes')
serious_outcomes = ['DE', 'LT', 'HO', 'DS', 'CA', 'RI']
print(f"Serious outcomes: {serious_outcomes}")

#compare serious to non-serious
outc['is_serious'] = outc['outc_cod'].isin(serious_outcomes).astype(int)
serious_count = outc['is_serious'].sum()
total_outcomes = len(outc)
serious_percent = (serious_count / total_outcomes) * 100

print(f"\nSerious outcomes: {serious_count:,} ({serious_percent:.1f}%)")
print(f"Non-Serious outcomes: {total_outcomes - serious_count:,} ({100 - serious_percent:.1f}%)")

"""### Observations from Outcomes Analysis

**Distribution**
- 44.6% serious outcomes (about 131,000 reports)- this will be the prediction target
  - Hospitalization is the most common serious outcome (27.5%)
  - Death is 9.5%
- 55.4% non-serious (about 163,000 reports)- labeled at 'Other'

**Data Structure Challenges:**
Multiple outcome codes can exist for a single report. For example, one primaryid has both DE and OT.
- The outcome table has 295,583 rows, but represents outcomes from 393,130 unique repots.
- Some patients have only one outcome code, others have multiple.
- We will flag a report as serious if it has any serious outcome.

The serious vs. non-serious outcome rates are well balanced for a classification model and it reflects the real world where serious outcomes are common but are not dominant. Next, we will merge the demographics and outcome tables. We will explore which demographic groups have higher serious outcome rates, or if age sex, or country affect serious outcomes. We also need to collapse to the report level since there can be mnultiple outcome codes per patient and flag a report as erious if it has any serious outcome code.

## 2.3 Merge Demographics with Outcomes

Now we connect patient demofraphics with outcomes to explore patterns:
- Which demographic groups have higher serious outcome rates?
- How do age, sex, and reporter country affect serious outcomes?

**Note**: Reports can have multiple outcomes codes. We will collapse to report level, flag as serious if it has any serioous outcome code.
"""

#merging DEMO with OUTC to analyze serious outcome patterns
print('Merging Demographics with Outcomes for Exploratory Analysis')
print('='*70)

#collapse OUTC to report level (one row per primaryid)
#A report is serious if any of its outcomes are serious
outc_report = outc.groupby('primaryid')['is_serious'].max().reset_index()

print(f"OUTC before collapsing: {len(outc):,} rows (multiple outcomes per report)")
print(f"OUTC after collapsing: {len(outc_report):,} rows (one row per report)\n")

#merge DEMO with collapsed outcomes- merges demographics with serious outcome flag
demo_outc = demo.merge(outc_report, on='primaryid', how='inner')

print(f"Merged dataset: {len(demo_outc):,} reports")
print(f"Reports with outcomes: {demo_outc['is_serious'].notna().sum():,}")

#overall serious outcome rate
print('\n' + '='*70)
print('Overall Serious Outcome Rate (Report Level):')
serious_count = demo_outc['is_serious'].sum()
total_reports = len(demo_outc)
serious_rate = (serious_count / total_reports) * 100

print(f"Serious: {serious_count:,} ({serious_rate:.1f}%)")
print(f"Non-serious: {total_reports - serious_count:,} ({100 - serious_rate:.1f}%)")

#Serious outcome rate by sex (excluding missing sex values)
print('\n' + '='*70)
print('Serious Outcome Rate by Sex:')
demo_outc_with_sex = demo_outc[demo_outc['sex'].notna()].copy()
sex_analysis = demo_outc_with_sex.groupby('sex')['is_serious'].agg([
    ('Total_Reports', 'count'),
    ('Serious_Count', 'sum'),
    ('Serious_Rate_%', lambda x: (x.mean() * 100).round(3))
])
print(sex_analysis)
print(f"\nNote: {len(demo_outc) - len(demo_outc_with_sex):,} reports excluded due to missing sex data")

#serious outcome rate by age group
print('\n' + '='*70)
print('Serious Outcome Rate by Age Group:')
#filter to reports with age data
demo_outc_with_age = demo_outc[demo_outc['age'].notna()].copy()

#create age categories
demo_outc_with_age['age_category'] = pd.cut(
    demo_outc_with_age['age'],
    bins=[0, 18, 45, 65, 150],
    labels=['0-17 (Children)', '18-44 (Adults)', '45-64 (Middle Age)', '65+ (Elderly)']
)

age_analysis = demo_outc_with_age.groupby('age_category')['is_serious'].agg([
    ('Total_Reports', 'count'),
    ('Serious_Count', 'sum'),
    ('Serious_Rate_%', lambda x: (x.mean() * 100).round(1))
])
print(age_analysis)
print(f"\nNote: Age analysis based on {len(demo_outc_with_age):,} reports with age data")
print(f"Excluded {len(demo_outc) - len(demo_outc_with_age):,} reports due to missing age data")

#top 10 reporter countries by serious outcome rate
print('\n' + '='*70)
print('Top 10 Reporter Countries by Volume:')
country_analysis = demo_outc.groupby('reporter_country')['is_serious'].agg([
    ('Total_Reports', 'count'),
    ('Serious_Count', 'sum'),
    ('Serious_Rate_%', lambda x: (x.mean() * 100).round(1))
]).sort_values('Total_Reports', ascending=False).head(10)
print(country_analysis)

"""### Key Findings from Demographic Analysis:

**Overall Serious Outcome Rate:**
- 50.6% of reports result in serious outcomes (112,527 serious vs 109,760 non-serious)
- Well-balanced dataset for modeling
- Note: This is at the report level - each patient counted once

**Sex Differences:**
- Males: 55.6% serious outcome rate
- Females: 47.9% serious outcome rate
- **Key Finding:** Males have 7.7 percentage point higher serious outcome rate than females
- Sex is a significant predictor and will be included in modeling
- 42,604 reports (19.2%) excluded due to missing sex data

**Age-Related Patterns:**
- **Clear age gradient:**
  - Young adults (18-44): 47.0% serious (lowest risk group)
  - Middle-age (45-64): 50.8% serious
  - **Elderly (65+): 59.5% serious (highest risk group)**
  - Children (0-17): 53.5% serious (surprisingly high)
- **Key Finding:** Elderly patients have 12.5 percentage point higher serious outcome rate than young adults
- Age is the strongest demographic predictor of serious outcomes
- 75,834 reports (34.1%) excluded due to missing age data

**Geographic Variation:**
- **Top reporting countries:** US (95,467 reports), Canada (22,323), Japan (14,564)
- **Serious outcome rates vary by country:**
  - Highest (out of top 10 reporting countries): France (62.8%), Germany (57.2%), Spain (57.4%)
  - Lowest (out of top 10 reporting countries): Australia (41.5%), UK (42.8%), Canada (43.9%)
  - US: 49.1%
- Country differences likely reflect reporting practices rather than drug safety differences
  - Some countries mandate reporting of serious events
  - Others have more voluntary reporting of minor events

**Implications for Modeling:**
1. **Age and sex are critical features** - clear relationship with serious outcomes
2. **Missing data is substantial:** 34% missing age, 19% missing sex
   - Will filter to complete cases in Section 3 (about 146K reports with both age and sex)
3. **Class balance:** 50.6% vs 49.4% requires no special sampling techniques
4. **Country may be useful** but should be interpreted as reporting bias, not drug risk

**Next Step:** Clean and merge all tables to create the final modeling dataset with demographic features, drug counts, and reaction counts.

## 2.4 Drug patterns Overview
Quick look at most reported drugs
"""

#quick drug eda
#top 20 most reported drugs
print("Top 20 Most Reported Drugs:")
print(drug['drugname'].value_counts().head(20))

#drugs per patient
drug_per_patient = drug.groupby('primaryid').size()
print("\nDrugs per patient distribution:")
print(drug_per_patient.describe())

"""# 3. Data Cleaning and Preparation

**Goal:** Create a clean patient-level dataset for modeling with:
- 1 row per patient
- Demographic features: age, sex, country
- Outcome flag: serious (1) or non-serious (0)
- Drug features: drug count, top drug flags
- Reaction features: reaction count

**Approach:** Collapse each table to patient-level brfore merging to avoid data duplication.

---

## 3.1 Collapse Outcomes to Patient Level

Already did this in Section 2.3 for exploration. Now we will create the official version for modeling.
"""

def clean_column_names(df):
    """Strip spaces, uppercase, and remove punctuation from column names."""
    df.columns = df.columns.str.strip().str.upper().str.replace(r'[^A-Z0-9_]', '', regex=True)
    return df

# Clean all tables
demo = clean_column_names(demo)
drug = clean_column_names(drug)
reac = clean_column_names(reac)
outc = clean_column_names(outc)
indi = clean_column_names(indi)
ther = clean_column_names(ther)
rpsr = clean_column_names(rpsr)

# Check cleaned columns
print("Cleaned Column Names:")
print("DEMO:", demo.columns.tolist())
print("DRUG:", drug.columns.tolist())
print("REAC:", reac.columns.tolist())
print("OUTC:", outc.columns.tolist())
print("INDI:", indi.columns.tolist())
print("THER:", ther.columns.tolist())
print("RPSR:", rpsr.columns.tolist())

# Records without a PRIMARYID cannot be linked to the case report, so they are dropped.
demo = demo.dropna(subset=['PRIMARYID'])
drug = drug.dropna(subset=['PRIMARYID'])
reac = reac.dropna(subset=['PRIMARYID'])
outc = outc.dropna(subset=['PRIMARYID'])
indi = indi.dropna(subset=['PRIMARYID'])
ther = ther.dropna(subset=['PRIMARYID'])
rpsr = rpsr.dropna(subset=['PRIMARYID'])

#Drop rows in 'demo' where 'age' OR 'sex' are missing
#DEMO table ONLY - as dropping from all tables will disrupt results
COLUMNS_TO_CHECK = ['AGE', 'SEX']

if all(col in demo.columns for col in COLUMNS_TO_CHECK):
    original_demo_len = len(demo)
    demo = demo.dropna(subset=COLUMNS_TO_CHECK)
    print(f"--- Step 2: Dropped {original_demo_len - len(demo):,} rows from 'demo' due to missing age or sex. ---")
else:
    print(f"--- Step 2: WARNING: Skipping age/sex cleaning. Expected columns {COLUMNS_TO_CHECK} not found in 'demo'. ---")

# Remove duplicates
print(" Removing duplicates from all 7 tables")

# For DEMO: Remove rows that are IDENTICAL across *all* columns.
# Need to keep multiple unique report rows with the same PRIMARYID.
print("DEMO: Dropping rows that are exact duplicates across ALL columns (not just PRIMARYID).")
demo = demo.drop_duplicates()

# Drop all duplicates from the other tables (already standard practice)
drug = drug.drop_duplicates()
reac = reac.drop_duplicates()
outc = outc.drop_duplicates()
indi = indi.drop_duplicates()
ther = ther.drop_duplicates()
rpsr = rpsr.drop_duplicates()

# ========================================================================
# Final Check
print("\n" + "="*60)
print("Final Row Counts After Cleaning All 7 Tables:")
# Row counts will reflect all the cleaning steps applied above
print(f"DEMO: {len(demo):,} rows")
print(f"DRUG: {len(drug):,} rows")
print(f"REAC: {len(reac):,} rows")
print(f"OUTC: {len(outc):,} rows")
print(f"INDI: {len(indi):,} rows")
print(f"THER: {len(ther):,} rows")
print(f"RPSR: {len(rpsr):,} rows")
print("="*60)

if 'ROLE_COD' in drug.columns:
    drug = drug[drug['ROLE_COD'].astype(str).str.upper() == 'PS']

print(f"Filtered to {len(drug):,} primary suspect drug records")

# Serious outcomes per FAERS documentation: DE, LT, HO, DS, CA, RI
serious_codes = ['DE', 'LT', 'HO', 'DS', 'CA', 'RI']

# Create binary column
outc['SERIOUS'] = outc['OUTC_COD'].isin(serious_codes).astype(int)

# Aggregate to PRIMARYID level (if any outcome is serious -> serious=1)
outcome_flags = outc.groupby('PRIMARYID')['SERIOUS'].max().reset_index()

print(outcome_flags['SERIOUS'].value_counts())

# Merge DEMO + DRUG (primary suspect only)
merged = pd.merge(demo, drug, on='PRIMARYID', how='inner')

# Merge with OUTCOME flag
merged = pd.merge(merged, outcome_flags, on='PRIMARYID', how='left')

# Merge with THERAPY dates
merged = pd.merge(merged, ther[['PRIMARYID', 'START_DT', 'END_DT']], on='PRIMARYID', how='left')

# Fill missing serious with 0 (non-serious)
merged['SERIOUS'] = merged['SERIOUS'].fillna(0).astype(int)

print(f"Merged dataset shape: {merged.shape}")
print("Serious outcome ratio:", merged['SERIOUS'].mean().round(3))

# Example: map SEX, AGE, and COUNTRY to numeric/categorical
if 'AGE' in merged.columns:
    merged['AGE'] = pd.to_numeric(merged['AGE'], errors='coerce')

if 'SEX' in merged.columns:
    merged['SEX'] = merged['SEX'].replace({'M': 1, 'F': 0, 'UNK': np.nan})

if 'GNDR_COD' in merged.columns:
    merged['SEX'] = merged['GNDR_COD'].replace({'M': 1, 'F': 0, 'U': np.nan})

# Example: Simplify REPORT_COUNTRY to top 10 only
if 'OCCP_COD' in merged.columns:
    top_countries = merged['OCCP_COD'].value_counts().nlargest(10).index
    merged['OCCP_COD'] = merged['OCCP_COD'].where(merged['OCCP_COD'].isin(top_countries), 'OTHER')

#Handle Missing Values
missing_summary = merged.isna().mean().sort_values(ascending=False)
print("Missing value ratio per column (top 10):")
print(missing_summary.head(10))

# Drop highly missing columns (>80% missing)
merged = merged.loc[:, missing_summary < 0.8]

merged.to_csv("FAERS25Q2_CLEANED.csv", index=False)
print("Cleaned FAERS dataset saved as FAERS25Q2_CLEANED.csv")

df = pd.read_csv("FAERS25Q2_CLEANED.csv")
# Merge with REAC table to get reaction information
reac = pd.read_csv('REAC25Q2.txt', encoding='utf-8', low_memory=False, sep='$', quotechar='"', on_bad_lines='skip')
reac.columns = reac.columns.str.strip().str.upper().str.replace(r'[^A-Z0-9_]', '', regex=True)
df = pd.merge(df, reac[['PRIMARYID', 'PT']], on='PRIMARYID', how='left')

print("Dataset shape:", df.shape)
df.head()

"""# 4. Feature Engingeering"""

#Age Groups
df['AGE_GROUP'] = pd.cut(df['AGE'], bins=[0,18,40,65,120], labels=['Child','YoungAdult','Adult','Senior'])

#Drug count per report
drug_count = df.groupby('PRIMARYID')['DRUGNAME'].count().reset_index()
drug_count.columns = ['PRIMARYID','NUM_DRUGS']
df = pd.merge(df, drug_count, on='PRIMARYID', how='left')

#Reaction count per report
reac_count = df.groupby('PRIMARYID')['PT'].count().reset_index()
reac_count.columns = ['PRIMARYID','NUM_REACTIONS']
df = pd.merge(df, reac_count, on='PRIMARYID', how='left')

#Code categorical variables
categorical_cols = ['SEX','ROUTE','RPSR_COD','AGE_GROUP','INDI_PT']
for col in categorical_cols:
    if col in df.columns:
        if df[col].dtype.name == 'category':
            df[col] = df[col].astype(object) # Convert to object to allow adding 'UNK'
        df[col] = df[col].fillna('UNK')
        df[col] = df[col].astype(str) # Convert to string for one-hot encoding

# Ensure 'RPSR_COD' and 'INDI_PT' are included if they exist after previous steps
# Check if 'RPSR_COD' exists before adding to categorical_cols
if 'RPSR_COD' not in df.columns and 'rpsr_cod' in df.columns:
    df.rename(columns={'rpsr_cod': 'RPSR_COD'}, inplace=True)
    if 'RPSR_COD' not in categorical_cols:
        categorical_cols.append('RPSR_COD')

# Check if 'INDI_PT' exists before adding to categorical_cols
if 'INDI_PT' not in df.columns and 'indi_pt' in df.columns:
    df.rename(columns={'indi_pt': 'INDI_PT'}, inplace=True)
    if 'INDI_PT' not in categorical_cols:
        categorical_cols.append('INDI_PT')

# Filter categorical_cols to only include columns actually present in df
categorical_cols = [col for col in categorical_cols if col in df.columns]


df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Drop unneeded columns
drop_cols = ['PRIMARYID','DRUGNAME','PT','START_DT','END_DT','OUTC_COD']
df_model = df.drop(columns=[c for c in drop_cols if c in df.columns])

print("Feature-engineered dataset shape:", df_model.shape)
df_model.head()

"""#5 Models and Metrics

##5.1 Logistic Regression
"""

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# separate features and target
X = df_model.drop('SERIOUS', axis=1)
y = df_model['SERIOUS']

print(f"Original data: {X.shape[0]:,} samples, {X.shape[1]} features")
print(f"Target distribution:\n{y.value_counts()}")
print(f"Serious rate: {y.mean():.3f}")

# handle categorical variables and missing values
def preprocess_features(X):
    """Convert all features to numeric and handle missing values"""
    X_processed = X.copy()

    # identify numeric and categorical columns
    numeric_cols = X_processed.select_dtypes(include=[np.number]).columns
    categorical_cols = X_processed.select_dtypes(include=['object', 'category']).columns

    print(f"Numeric columns: {len(numeric_cols)}")
    print(f"Categorical columns: {len(categorical_cols)}")

    # convert categorical columns to numeric using Label Encoding
    label_encoders = {}
    for col in categorical_cols:
        le = LabelEncoder()
        # handle missing values by treating them as a separate category
        X_processed[col] = X_processed[col].astype(str).fillna('MISSING')
        X_processed[col] = le.fit_transform(X_processed[col])
        label_encoders[col] = le

    # handles missing values in numeric columns
    imputer = SimpleImputer(strategy='median')
    X_processed[numeric_cols] = imputer.fit_transform(X_processed[numeric_cols])

    return X_processed, label_encoders

# preprocess the features
X_processed, label_encoders = preprocess_features(X)

print(f"\nAfter preprocessing: {X_processed.shape[1]} features")
print(f"Data types:\n{X_processed.dtypes.value_counts()}")

# split
X_train, X_test, y_train, y_test = train_test_split(
    X_processed, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nTraining set: {X_train.shape[0]:,} samples")
print(f"Test set: {X_test.shape[0]:,} samples")

# scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"Features scaled: {X_train_scaled.shape[1]} features")

# initialize and train logistic regression
logr = LogisticRegression(
    random_state=42,
    max_iter=1000,
    class_weight='balanced',
    C=1.0,
    solver='liblinear'
)

print("\nTraining logistic regression...")
logr.fit(X_train_scaled, y_train)

# make predictions
y_pred = logr.predict(X_test_scaled)
y_pred_proba = logr.predict_proba(X_test_scaled)[:, 1]  # Probability scores

# evaluate the model
print("=" * 60)
print("LOGISTIC REGRESSION RESULTS")
print("=" * 60)

# basic metrics
accuracy = accuracy_score(y_test, y_pred)
balanced_acc = balanced_accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)

print(f"Accuracy: {accuracy:.4f}")
print(f"Balanced Accuracy: {balanced_acc:.4f}")
print(f"ROC AUC: {roc_auc:.4f}")

# precision, Recall, F1
precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")

# classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['Non-Serious', 'Serious']))

# feature Importance
print("\nTop 15 Most Important Features:")
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'coefficient': logr.coef_[0],
    'abs_importance': np.abs(logr.coef_[0])
}).sort_values('abs_importance', ascending=False)

print(feature_importance.head(15))

# odds Ratios for interpretation
feature_importance['odds_ratio'] = np.exp(feature_importance['coefficient'])
print("\nTop 10 Features by Odds Ratio (>1 increases serious risk):")
top_odds = feature_importance.nlargest(10, 'abs_importance')[['feature', 'coefficient', 'odds_ratio']]
print(top_odds)

# confusion Matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Non-Serious', 'Serious'])
disp.plot(cmap='Blues', values_format='d')
plt.title('Logistic Regression - Confusion Matrix')
plt.show()

#  Cross-Validation for robustness
from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(logr, X_train_scaled, y_train, cv=5, scoring='roc_auc')
print(f"\nCross-Validation ROC AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

"""Tarif

The logistic regression model successfully predicts serious adverse drug events with 71.3% accuracy and 80.2% ROC AUC, demonstrating strong predictive capability for identifying high-risk drug safety reports.


*   Accuracy	- 71.29% - Overall correct prediction rate
*   Balanced Accuracy	- 71.44% -	Accounts for class imbalance
*   ROC AUC	- 80.21%
*   Precision	- 65.55%	- When predicting serious, 65.6% are correct
*   Recall	- 72.63%	- Captures 72.6% of actual serious events
*   Recall	- 72.63%	- Captures 72.6% of actual serious events
*   Recall	- 72.63% - Captures 72.6% of actual serious events
*   F1-Score	- 68.91%	- Balanced measure of precision/recall

**Confusion Matrix**
* Sensitivity (Recall): 72.6% of serious events correctly identified

* Specificity: 70.2% of non-serious events correctly identified

The matrix shows a good balance between catching serious events and avoiding false alarms

**Strong Predictive Feartres accroding to the model**
* NUMBER OF DRUGS : Patients taking multiple drugs have dramatically higher serious event risk

* DOSE AMOUNT : Higher doses associated with 133% increased serious event risk

* ROUTE OF ADMINISTRATION: Oral: 27.8% increased risk, intravenous: 22.8 increased risk, subcutaneous: 14.9% increased risk

* PATIENT CHARACTERISTICS: Older patients at higher risk and weight was a significant predictor

**Model Strength**
* Discriminatory Power (AUC = 0.802)
* Class Balance Handling (Balanced accuracy = 0.714)
* Robust Performance (Cross-validation AUC = 0.803 ± 0.003)

##5.2  Decision Trees & RandomForests
"""

#Import Algorithms and Metrics
from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()

from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
rforest = RandomForestClassifier()
gradboost = GradientBoostingClassifier()

#Serious Outcomes
serious_codes = ['DE', 'LT', 'HO', 'DS', 'CA', 'RI']
outc['SERIOUS'] = outc['OUTC_COD'].isin(serious_codes).astype(int)
outcome_flags = outc.groupby('PRIMARYID')['SERIOUS'].max().reset_index()

#Merging with cleaned DEMO (with AGE/SEX) and DRUG (PS only)
merged = pd.merge(demo, drug, on='PRIMARYID', how='inner')
merged = pd.merge(merged, outcome_flags, on='PRIMARYID', how='left')
merged = pd.merge(merged, ther[['PRIMARYID', 'START_DT', 'END_DT']], on='PRIMARYID', how='left')
merged['SERIOUS'] = merged['SERIOUS'].fillna(0).astype(int)
merged['AGE'] = pd.to_numeric(merged['AGE'], errors='coerce')
merged['SEX'] = merged['SEX'].replace({'M': 1, 'F': 0, 'UNK': np.nan})

# Count Features (Reaction Count, Indication Count)
reac_count = reac.groupby('PRIMARYID').size().reset_index(name='REAC_COUNT')
merged_features = merged.merge(reac_count, on='PRIMARYID', how='left')
indi_count = indi.groupby('PRIMARYID').size().reset_index(name='INDI_COUNT')
merged_features = merged_features.merge(indi_count, on='PRIMARYID', how='left')
merged_features['REAC_COUNT'] = merged_features['REAC_COUNT'].fillna(0)
merged_features['INDI_COUNT'] = merged_features['INDI_COUNT'].fillna(0)

# Aggregate to Report Level (PrimaryID)
model_df = merged_features.drop_duplicates(subset=['PRIMARYID']).copy()
model_df.dropna(subset=['AGE', 'SEX'], inplace=True)

# Categorical Feature Encoding
le_drug = LabelEncoder()
model_df['DRUGNAME_ENCODED'] = le_drug.fit_transform(model_df['DRUGNAME'].astype(str))
top_10_countries = model_df['REPORTER_COUNTRY'].value_counts().head(10).index.tolist()
model_df['REPORTER_COUNTRY_GROUPED'] = model_df['REPORTER_COUNTRY'].apply(
    lambda x: x if x in top_10_countries else 'OTHER'
)
country_dummies = pd.get_dummies(model_df['REPORTER_COUNTRY_GROUPED'], prefix='Ctry', drop_first=True)
X = model_df.drop(columns=['PRIMARYID', 'SERIOUS', 'REPORTER_COUNTRY', 'REPORTER_COUNTRY_GROUPED', 'DRUGNAME'])
X = pd.concat([X, country_dummies], axis=1)
y = model_df['SERIOUS']

# Apply preprocessing to X to handle remaining categorical and missing values
X_processed, _ = preprocess_features(X)

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42, stratify=y)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Decision Tree Model (Baseline)
dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)
dt_model.fit(X_train, y_train)
y_pred_dt = dt_model.predict(X_test)
y_proba_dt = dt_model.predict_proba(X_test)[:, 1]

# Random Forest Model (Robust Predictor)
rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, class_weight='balanced', n_jobs=-1)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)
y_proba_rf = rf_model.predict_proba(X_test)[:, 1]

# Feature Importance
feature_importances = pd.Series(rf_model.feature_importances_, index=X_train.columns)
top_10_features = feature_importances.nlargest(10)

# --- 4. Evaluation (Final Metrics) ---
dt_auc = roc_auc_score(y_test, y_proba_dt)
rf_auc = roc_auc_score(y_test, y_proba_rf)
rf_report = classification_report(y_test, y_pred_rf, output_dict=True)

print("Models Trained Successfully.")

print("=" * 60)
print("DECISION TREE RESULTS")
print("=" * 60)

# Basic metrics for Decision Tree
accuracy_dt = accuracy_score(y_test, y_pred_dt)
balanced_acc_dt = balanced_accuracy_score(y_test, y_pred_dt)
roc_auc_dt = roc_auc_score(y_test, y_proba_dt)
precision_dt, recall_dt, f1_dt, _ = precision_recall_fscore_support(y_test, y_pred_dt, average='binary')

print(f"Accuracy: {accuracy_dt:.4f}")
print(f"Balanced Accuracy: {balanced_acc_dt:.4f}")
print(f"ROC AUC: {roc_auc_dt:.4f}")
print(f"Precision: {precision_dt:.4f}")
print(f"Recall: {recall_dt:.4f}")
print(f"F1-Score: {f1_dt:.4f}")

# Classification Report
print("\nClassification Report (Decision Tree):")
print(classification_report(y_test, y_pred_dt, target_names=['Non-Serious', 'Serious']))

# Confusion Matrix
plt.figure(figsize=(8, 6))
cm_dt = confusion_matrix(y_test, y_pred_dt)
disp_dt = ConfusionMatrixDisplay(confusion_matrix=cm_dt, display_labels=['Non-Serious', 'Serious'])
disp_dt.plot(cmap='Oranges', values_format='d')
plt.title('Decision Tree - Confusion Matrix')
plt.show()

"""##5.3 KNN"""

#Import Algorithms and Metrics
from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()

# Initialize and train KNN model
knn_model = KNeighborsClassifier(n_neighbors=5, n_jobs=-1) # Using 5 neighbors as a starting point
knn_model.fit(X_train_scaled, y_train)

# Make predictions
y_pred_knn = knn_model.predict(X_test_scaled)
y_pred_proba_knn = knn_model.predict_proba(X_test_scaled)[:, 1]

# Evaluate the KNN model
print("=" * 60)
print("K-NEAREST NEIGHBORS RESULTS")
print("=" * 60)

accuracy_knn = accuracy_score(y_test, y_pred_knn)
balanced_acc_knn = balanced_accuracy_score(y_test, y_pred_knn)
roc_auc_knn = roc_auc_score(y_test, y_pred_proba_knn)
precision_knn, recall_knn, f1_knn, _ = precision_recall_fscore_support(y_test, y_pred_knn, average='binary')

print(f"Accuracy: {accuracy_knn:.4f}")
print(f"Balanced Accuracy: {balanced_acc_knn:.4f}")
print(f"ROC AUC: {roc_auc_knn:.4f}")
print(f"Precision: {precision_knn:.4f}")
print(f"Recall: {recall_knn:.4f}")
print(f"F1-Score: {f1_knn:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred_knn, target_names=['Non-Serious', 'Serious']))

# Confusion Matrix
plt.figure(figsize=(8, 6))
cm_knn = confusion_matrix(y_test, y_pred_knn)
disp_knn = ConfusionMatrixDisplay(confusion_matrix=cm_knn, display_labels=['Non-Serious', 'Serious'])
disp_knn.plot(cmap='Greens', values_format='d')
plt.title('K-Nearest Neighbors - Confusion Matrix')
plt.show()

"""**Accuracy: 0.7713 (77.13%)**-
- Model correctly predicted the outcome (whether an event was 'Serious' or 'Non-Serious') for 77.13% of the cases in the test set.

**Balanced Accuracy: 0.7310 (73.10%)** -
- Average of recall obtained on each class. It's particularly useful when dealing with imbalanced classes, as it provides a more realistic view of the model's performance compared to simple accuracy. A score of 73.10% indicates that the model performs reasonably well across both 'Serious' and 'Non-Serious' classes.

**ROC AUC: 0.8313 -**
-  Model's ability to distinguish between the two classes
- Good

#**For 'Non-Serious' Events (Class 0):**

**Precision: 0.82 (82%)**
- When the model predicts an event is 'Non-Serious', it is correct 82% of the time.

**Recall: 0.85 (85%)**
- The model successfully identified 85% of all actual 'Non-Serious' events.

**F1-Score: 0.83**
- This is the harmonic mean of precision and recall, providing a balanced measure of the model's performance for 'Non-Serious' events.


#**For 'Serious' Events (Class 1):**

**Precision: 0.67 (67%)**
- When the model predicts an event is 'Serious', it is correct 67% of the time. This is important for drug safety, as it tells us how often we correctly flag a high-priority event.

**Recall: 0.61 (61%)**
- The model successfully identified 61% of all actual 'Serious' events. This metric is crucial because it tells us how many of the truly serious events the model 'caught'. A higher recall means fewer serious events are missed.

**F1-Score: 0.64**
 - A balanced measure of precision and recall for 'Serious' events.
Confusion Matrix Breakdown: The Confusion Matrix visually represents these outcomes:

**True Negatives (25,405)**:
-  The model correctly predicted 25,405 'Non-Serious' events as 'Non-Serious'.

**False Positives (4,474):**
- The model incorrectly predicted 4,474 'Non-Serious' events as 'Serious'. These are 'false alarms' – cases that might be unnecessarily investigated.

**False Negatives (5,744):**
- The model incorrectly predicted 5,744 'Serious' events as 'Non-Serious'. These are critical errors, as they represent missed 'Serious' events that might require immediate attention.

**True Positives (9,051):**
- The model correctly predicted 9,051 'Serious' events as 'Serious'. These are the correctly identified high-priority cases.
Summary:

The KNN model demonstrates good overall performance, with a strong ROC AUC score. It is quite effective at identifying 'Non-Serious' events. For 'Serious' events, it achieves a reasonable precision of 67% and a recall of 61%. This means that while it correctly flags a good portion of serious cases, there is still a significant number of actual serious events (5,744 in the test set) that the model failed to identify. Depending on the cost associated with false negatives (missing a serious event) versus false positives (investigating a non-serious event), you might tune the model or explore other algorithms to optimize for a higher recall of 'Serious' events.

#Results

## 6.1 Outputs and Analysis

#Sex Differences
##Males have 7.7 percentage point higher serious outcome rate than females
Males = 55.6% Serious
Females = 47.9% Serious

#Age
Young adults (18-44): 47.0% serious (lowest risk group)
Middle-age (45-64): 50.8% serious
Elderly (65+): 59.5% serious (highest risk group)
Children (0-17): 53.5% serious (surprisingly high)

#Main Predictive Features
Number of Drugs
Dose Amounts
Patient Demographics

#HO
Hospitalization is the Most Common Serious Outcome (~27%)

Death (DE)  follows with about 9.5%

#Our models are recommendations to predict whether an adverse drug event will result in a serious outcome
"""